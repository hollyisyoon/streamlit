import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from pyvis.network import Network
import networkx as nx
import gensim
from gensim.models import Word2Vec
from PIL import Image

import pandas as pd
import ast
import time
from datetime import datetime, timedelta
import itertools
from markdownlit import mdlit

import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags
import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud


# CSS ìŠ¤íƒ€ì¼ ì •ì˜
css_code = """
<style>
    .custom-sidebar {
        padding: 20px;
        background-color: #f2f2f2;
        font-size: 18px;
        color: #333;
    }
    
    .custom-sidebar a {
        color: #333;
        text-decoration: none;
    }
</style>
"""

STYLE = """
.callout {
    padding: 1em;
    border-radius: 0.5em;
    background-color: #F8F8F8;
    border-left: 4px solid #195ef7;
    margin-bottom: 1em;
    color: black;
}

.callout a#key1 {
    color: #000;
    background-color: #FAF3DD;
    text-decoration: none;
}

.callout a#key2 {
    color: #000;
    background-color: #E9F3F7;
    text-decoration: none;
}

.callout a#key3 {
    color: #000;
    background-color: #F6F3F8;
    text-decoration: none;
}

.callout a#key4 {
    color: #000;
    background-color: #EEF3ED;
    text-decoration: none;
}
"""

st.title("ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")

#########Section3 - í‚¤ì›Œë“œ deepdive(ì‹œê³„ì—´)############
st.markdown("---")
st.markdown("<h2 id='section3'>â³ ì‹œê¸°ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„</h2>", unsafe_allow_html=True)
df2 = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df2['ë‚ ì§œ'] = pd.to_datetime(df2['ë‚ ì§œ'])

col1, col2 = st.beta_columns((0.2, 0.8))
with col1:
    keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='ì œë¼ëŠ„')
with col2:
    keyword2 = st_tags(
        label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
        value = ['ìŠ¤í‚¨ë‹µì„œìŠ¤'],
        maxtags = 5,
        key = '2')

def get_df(df, word1, args):
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
    result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
    keywords = [word1] + (args)
    result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
    for arg in keywords:
        if arg not in ' '.join(result['ì œëª©+ë‚´ìš©(nng)'].tolist()):
            st.warning(f"'ë‹¤ìŒ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. {arg}'")
            return None, None
    return result, keywords

def deepdive_lineplot(df, keywords):
    # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    keywords = keywords[::-1]
    keyword_dfs = {}
    for keyword in keywords:
        keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    impact_by_week = {}
    for keyword, keyword_df in keyword_dfs.items():
        keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
        keyword_df.set_index('ë‚ ì§œ', inplace=True)
        impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

    # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    for i, (keyword, impact) in enumerate(impact_by_week.items()):
        fig.add_trace(go.Scatter(x=impact.index, y=impact.values, name=keyword, line_color=colors[i]), secondary_y=False)
        
    fig.update_layout(yaxis_title="í‰ê·  ì˜í–¥ë„")
    st.plotly_chart(fig, use_container_width=True)

def get_TOP_10(df, keyword):
    temp_df = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)]
    top10_list = []
    for media_category in temp_df['ë§¤ì²´'].unique():
        df_category = temp_df[temp_df['ë§¤ì²´'] == media_category]
        if len(df_category) > 0:
            try:
                band_top10 = df_category.nlargest(10, 'ì˜í–¥ë„')
                band_top10['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                band_top10 = band_top10.reset_index(drop=True)
                band_top10 = band_top10[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(band_top10)
            except ValueError:
                df_category['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                df_category = df_category.reset_index(drop=True)
                df_category = df_category[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(df_category)
    if len(top10_list) > 0:
        return pd.concat(top10_list, ignore_index=False)
    else:
        return None
    
try :
    st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
    st.markdown("<h3>í‚¤ì›Œë“œë³„ ì˜í–¥ë„ ê·¸ë˜í”„</h3>", unsafe_allow_html=True)
    deepdive_df, deepdive_keywords = get_df(df2, keyword1, keyword2)
    deepdive_lineplot(deepdive_df, deepdive_keywords)
    st.markdown("<h3>ë§¤ì²´ë³„ Top10 ê²Œì‹œê¸€</h3>", unsafe_allow_html=True)
    keyword_result = get_TOP_10(df2, keyword1)
    st.dataframe(keyword_result)

except :
    st.warning("í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")


#########Section4 - í‚¤ì›Œë“œ deepdive(ë„¤íŠ¸ì›Œí¬ ë¶„ì„)############
st.markdown("---")
st.markdown("<h2 id='section4'>í‚¤ì›Œë“œ ì—°ê´€íƒìƒ‰</h2>", unsafe_allow_html=True)

all_keywords = [keyword1]+keyword2
st.text(f'ğŸ”® {all_keywords}ì— ëŒ€í•œ ì—°ê´€ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤')

#ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê²°ê³¼
def ë„¤íŠ¸ì›Œí¬(network_list, all_keywords):
    networks = []
    for review in network_list:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=100)

    G = nx.Graph(font_path='/app/streamlit/font/NanumBarunGothic.ttf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in all_keywords:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=20)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in all_keywords:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    # plt.figure(figsize=(15,15))
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", font_weight="medium", edge_color="grey", width=edge_weights)

    # # ì¤‘ì‹¬ ë…¸ë“œë“¤ë¼ë¦¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶œë ¥
    # overlapping_í‚¤ì›Œë“œ = set()
    # for i, keyword1 in enumerate(all_keywords):
    #     for j, keyword2 in enumerate(all_keywords):
    #         if i < j and keyword1 in G and keyword2 in G:
    #             if nx.has_path(G, keyword1, keyword2):
    #                 overlapping_í‚¤ì›Œë“œ.add(keyword1)
    #                 overlapping_í‚¤ì›Œë“œ.add(keyword2)
    # if overlapping_í‚¤ì›Œë“œ:
    #     print(f"ë‹¤ìŒ ì¤‘ì‹¬ í‚¤ì›Œë“œë“¤ë¼ë¦¬ ì—°ê´€ì„±ì´ ìˆì–´ ì¤‘ë³µë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(overlapping_í‚¤ì›Œë“œ)}")

    net = Network(notebook=True, cdn_resources='in_line')
    net.from_nx(G)
    return [net, similar_words]

#ì—°ê´€ë¶„ì„
expander = st.expander('ì—°ê´€ë¶„ì„ ì„¸ë¶€í•„í„°')
with expander:
    col1, col2= st.beta_columns(2)    
    min_date = datetime(2022, 6, 1)
    max_date = datetime(2023, 4, 26)
    with col1:
        start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                                value=datetime(2022,6,1),
                                min_value=min_date,
                                max_value=max_date - timedelta(days=7))
        # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
        end_date = st.date_input("ë ë‚ ì§œ",
                                value=datetime(2022,7,1),
                                min_value=start_date + timedelta(days=7),
                                max_value=start_date + timedelta(days=60))
    with col2:
        media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'), help="í™•ì¸í•˜ê³  ì‹¶ì€ ì™¸ë¶€ ë°ì´í„°ì˜ ë§¤ì²´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def extract_df(df, media, start_date, end_date):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date)]
    return standard_df

df_ì—°ê´€ë¶„ì„ = extract_df(df2, media, start_date, end_date)

if st.button('ë¶„ì„ì„ ì‹œì‘í•˜ê¸°'):
    with st.spinner('ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        test2 = ['ì œë¼ëŠ„', 'í•´ì¶©', 'ì‘ì• ']
        test1 = [eval(i) for i in df_ì—°ê´€ë¶„ì„['ì œëª©+ë‚´ìš©(nng)']]
        try:
            ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(network_list, all_keywords)
            net = ë„¤íŠ¸ì›Œí¬[0]
            net.save_graph(f'/app/streamlit/pyvis_graph.html')
            HtmlFile = open(f'/app/streamlit/pyvis_graph.html', 'r', encoding='utf-8')
            components.html(HtmlFile.read(), height=600)
        except:
            st.warning('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')