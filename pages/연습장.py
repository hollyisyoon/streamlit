import streamlit as st
import streamlit.components.v1 as components
import pandas as pd

import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import ast
import time

import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
from datetime import datetime, timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")
import networkx as nx
from gensim.models import Word2Vec
import time
import itertools
from markdownlit import mdlit


st.title("Yellow component")

html_content = "<div>Hello world</div>"
yellow_background = "<style>:root {background-color: yellow;}</style>"
components.html(yellow_background + html_content)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
# df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)

def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    return standard_df, new_df

col1, col2, col3 = st.beta_columns(3)

min_date = datetime(2022, 6, 1)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2022,6,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2022,6,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=60))

with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'))

with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 80)
    effect_size = (100-int(temp_effect_size))/100

standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

def rising_keyword(standard_df, new_df):
    # ë°ì´í„° í•©ì¹˜ê¸° 
    df = pd.concat([standard_df, new_df])

    # ë‚ ì§œ êµ¬í•˜ê¸°
    ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚  = df['ë‚ ì§œ'].max()
    ì´ë²ˆì£¼ì²«ë‚  = (df['ë‚ ì§œ'].max() - timedelta(days=7))
    ì§€ë‚œì£¼ì²«ë‚  = ì´ë²ˆì£¼ì²«ë‚  - timedelta(days=7)
    
    ì´ë²ˆì£¼_df = df[(df['ë‚ ì§œ'] > ì´ë²ˆì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚ )]
    ì§€ë‚œì£¼_df = df[(df['ë‚ ì§œ'] > ì§€ë‚œì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ì²«ë‚ )]
        
    # ì¤‘ë³µê°’ ì œê±°í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€
    ì´ë²ˆì£¼_df = ì´ë²ˆì£¼_df.copy()
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    ì§€ë‚œì£¼_df = ì§€ë‚œì£¼_df.copy()
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    this_week_words = list(ì´ë²ˆì£¼_df['unique_content'].explode())
    last_week_words = list(ì§€ë‚œì£¼_df['unique_content'].explode())

    this_week_word_counts = Counter(this_week_words)
    last_week_word_counts = Counter(last_week_words)

    # ì´ë²ˆì£¼ì™€ ì§€ë‚œì£¼ì— ëª¨ë‘ ì–¸ê¸‰ëœ ë‹¨ì–´ë¥¼ ëª¨ì€ ì§‘í•©
    common_words = set(this_week_word_counts.keys()) & set(last_week_word_counts.keys())
    result = {}
    for word in common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ëª¨ë“  URLì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        url_list = list(ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: word in x)]['URL'])
        # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ URLì„ ì°¾ì•„ì„œ ì¶œë ¥
        url = max(url_list, key=lambda x: ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['URL'] == x, 'ì˜í–¥ë„'].iloc[0])
        increase_rate = (this_week_word_counts[word] - last_week_word_counts[word]) / this_week_word_counts[word]
        result[word] = {'ìƒìŠ¹ë¥ ': round(increase_rate, 2), 'URL': url}

    # ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
    keywords = []
    ups = []
    urls = []
    titles = []

    for word, data in sorted(result.items(), key=lambda x: x[1]['ìƒìŠ¹ë¥ '], reverse=True):
        if data['ìƒìŠ¹ë¥ ']>0:
            keywords.append(word)
            ups.append(f"{data['ìƒìŠ¹ë¥ ']*100}%")
            urls.append(data['URL'])
            titles.append(data['ì œëª©'])

    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'ìƒìŠ¹ë¥ ': ups,
        'URL': urls,
        'title': titles
    })

    if len(result_df.index) >= 1 :
        return result_df
    
st.subheader('ğŸ”¥ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ')
try:
    rising_keyword = rising_keyword(standard_df, new_df)
    rising_keyword
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")