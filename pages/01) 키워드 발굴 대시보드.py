import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pyvis.network import Network
import networkx as nx
import gensim
from gensim.models import Word2Vec
from PIL import Image

import pandas as pd
import ast
import time
from datetime import datetime, timedelta
import itertools
from markdownlit import mdlit

#ìŠ¤íŠ¸ë¦¼ì‡
import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags
import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

#ê³„ì‚°
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud


# CSS ìŠ¤íƒ€ì¼ ì •ì˜
css_code = """
<style>
    .custom-sidebar {
        padding: 20px;
        background-color: #f2f2f2;
        font-size: 18px;
        color: #333;
    }
    
    .custom-sidebar a {
        color: #333;
        text-decoration: none;
    }
</style>
"""

STYLE = """
.callout {
    padding: 1em;
    border-radius: 0.5em;
    background-color: #F8F8F8;
    border-left: 4px solid #195ef7;
    margin-bottom: 1em;
    color: black;
}

h3 {
    font-size: 1.5em;
    font-weight: bold;
    color: #143bff;
    padding: 0.3em;
    margin: 0.3em;
    background-color: #f5f5f5;
    border-radius: 10px;
}
"""

##############ë©”ì¸ ì½˜í…ì¸ 
st.title("ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")

#########Section1 - wordcloud############
st.markdown("<h2 id='section1'>ğŸª„ í‚¤ì›Œë“œ ë°œêµ´</h2>", unsafe_allow_html=True)

##ë°ì´í„°##
def to_list(text):
    return ast.literal_eval(text)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]    
    return standard_df, new_df

##ì¸í’‹ í•„í„° - ì „ì²´ìš©##
col1, col2, col3 = st.beta_columns(3)
min_date = datetime(2022, 6, 1)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2022,6,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2022,6,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=60))
with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'), help="í™•ì¸í•˜ê³  ì‹¶ì€ ì™¸ë¶€ ë°ì´í„°ì˜ ë§¤ì²´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 83, help="ê° ë§¤ì²´ë³„ ì½˜í…ì¸ ì˜ ë°˜ì‘ë„ë¥¼ ì ìˆ˜í™”í•œ ê°’ì…ë‹ˆë‹¤. 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì˜í–¥ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
    effect_size = (100-int(temp_effect_size))/100
standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

##ì¸í’‹ í•„í„° - ì›Œë“œí´ë¼ìš°ë“œìš©##
expander = st.expander('ì›Œë“œ í´ë¼ìš°ë“œ ì„¸ë¶€í•„í„°')
with expander:
    col1, col2= st.beta_columns(2)    
    with col1:
        type = st.selectbox('ê¸°ì¤€',('ë‹¨ìˆœ ë¹ˆë„(Countvectorizer)','ìƒëŒ€ ë¹ˆë„(TF-IDF)'), 
                            help="""ë‹¨ìˆœë¹ˆë„ë€ ë¬¸ì„œ ë‚´ ê° ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ë¹ˆë„ ì¦‰, ë‚˜íƒ€ë‚œ íšŸìˆ˜ë¥¼ ì„¸ì„œ ë§Œë“  ê°’ì…ë‹ˆë‹¤. 
                            ìƒëŒ€ë¹ˆë„ëŠ” ë‹¨ì–´ê°€ ë¬¸ì„œ ë‚´ì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.""")
    with col2:
        keyword_no = st.number_input("í‚¤ì›Œë“œ ë³¼ë¥¨", value=100, min_value=1, step=1,
                                     help="ì›Œë“œ í´ë¼ìš°ë“œë¥¼ í†µí•´ ë³´ê³  ì‹¶ì€ ë‹¨ì–´ì˜ ê°¯ìˆ˜ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")   
    stopwords = st_tags(
        label = 'ì œê±°í•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”',
        value = ['ì‹ë¬¼', 'í™”ë¶„'],
        suggestions = ['ì‹ë¬¼', 'í™”ë¶„'],
        key = '1')

##ì›Œë“œ í´ë¼ìš°ë“œ##
def get_tfidf_top_words(df, keyword_no):
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    tfidf_top_words = dict(tfidf_top_words)
    return tfidf_top_words

def get_count_top_words(df, keyword_no):
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    return count_top_words

try :
    if type == 'ë‹¨ìˆœ ë¹ˆë„(Countvecterize)' :
        words = get_count_top_words(standard_df, keyword_no)
    else :
        words = get_tfidf_top_words(standard_df, keyword_no)

    #ì›Œë“œí´ë¼ìš°ë“œ
    wc = WordCloud(background_color="white", colormap='Spectral', contour_color='steelblue', font_path="/app/streamlit/font/Pretendard-Bold.otf")
    wc.generate_from_frequencies(words)

    ## ë™ì  ì›Œë“œ í´ë¼ìš°ë“œ ##
    # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„±
    word_list = []
    freq_list = []
    fontsize_list = []
    position_list = []
    orientation_list = []
    color_list = []

    for (word, freq), fontsize, position, orientation, color in wc.layout_:
        word_list.append(word)
        freq_list.append(freq)
        fontsize_list.append(fontsize)
        position_list.append(position)
        orientation_list.append(orientation)
        color_list.append(color)

    # get the positions
    x = []
    y = []
    for i in position_list:
        x.append(i[0])
        y.append(i[1])

    # WordCloud ì‹œê°í™”ë¥¼ ìœ„í•œ Scatter Plot ìƒì„±
    hover_text = word_list  # í‚¤ì›Œë“œë§Œ í¬í•¨í•œ í…ìŠ¤íŠ¸ ìƒì„±

    # ìˆ«ìê°’ì„ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ hovertemplate ì„¤ì •
    hover_template = "<b>%{text}</b><br>Count: %{customdata[0]}"

    fig = go.Figure(go.Scatter(
        x=x, y=y, mode="text",
        text=hover_text,
        customdata=list(zip(freq_list)),  # ìˆ«ìê°’ì„ customdataë¡œ ì „ë‹¬
        hovertemplate=hover_template,  # hovertemplate ì„¤ì •
        textfont=dict(size=fontsize_list, color=color_list),
    ))

    fig.update_layout(
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        hovermode='closest'
    )

    st.plotly_chart(fig, use_container_width=True)

except :
    st.warning('ì˜í–¥ë„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”! ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤')    

#########Section2 - í‚¤ì›Œë“œ íë ˆì´íŒ…############
st.markdown("---")
st.markdown("<h2 id='section2'>ğŸ’ í‚¤ì›Œë“œ íë ˆì´ì…˜</h2>", unsafe_allow_html=True)
def new_keyword(standard_df, new_df):
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    content_list_1 = []
    content_list_1.extend(list(itertools.chain.from_iterable([eval(i) for i in standard_df['ì œëª©+ë‚´ìš©(nng)']])))
    content_list_2 = []
    content_list_2.extend(list(itertools.chain.from_iterable([eval(i) for i in new_df['ì œëª©+ë‚´ìš©(nng)']])))

    new_keywords = set(content_list_2) - set(content_list_1)   
    result_dict = {}
    # ì´ë²ˆë‹¬ì—ë§Œ ìˆëŠ” 
    for word in new_keywords:
        word_df = new_df[new_df['ì œëª©+ë‚´ìš©(nng)'].str.contains(word)]
        if len(word_df) > 0:
            avg_views = word_df['ì˜í–¥ë„'].mean()
            urls = word_df['URL'].tolist()
            result_dict[word] = {'í‰ê·  ì˜í–¥ë„': round(float(avg_views), 2), 'URL': urls}
            
    # ì¡°íšŒìˆ˜ ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬        
    result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1]['í‰ê·  ì˜í–¥ë„'], reverse=True))    

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    keywords = []
    avg_views = []
    urls = []
    
    for key, value in result_dict.items():
        keywords.append(key)
        avg_views.append(value['í‰ê·  ì˜í–¥ë„'])
        urls.append('\n'.join(value['URL']))
    
    result_df = pd.DataFrame({
        'í‰ê·  ì˜í–¥ë„': avg_views,
        'í‚¤ì›Œë“œ': keywords,
        'URL': urls
    })
    return result_df

def rising_keyword(standard_df, new_df):
    # ë°ì´í„° í•©ì¹˜ê¸° 
    df = pd.concat([standard_df, new_df])

    # ë‚ ì§œ êµ¬í•˜ê¸°
    ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚  = df['ë‚ ì§œ'].max()
    ì´ë²ˆì£¼ì²«ë‚  = (df['ë‚ ì§œ'].max() - timedelta(days=7))
    ì§€ë‚œì£¼ì²«ë‚  = ì´ë²ˆì£¼ì²«ë‚  - timedelta(days=7)
    
    ì´ë²ˆì£¼_df = df[(df['ë‚ ì§œ'] > ì´ë²ˆì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚ )]
    ì§€ë‚œì£¼_df = df[(df['ë‚ ì§œ'] > ì§€ë‚œì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ì²«ë‚ )]
        
    # ì¤‘ë³µê°’ ì œê±°í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€
    ì´ë²ˆì£¼_df = ì´ë²ˆì£¼_df.copy()
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    ì§€ë‚œì£¼_df = ì§€ë‚œì£¼_df.copy()
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    this_week_words = list(ì´ë²ˆì£¼_df['unique_content'].explode())
    last_week_words = list(ì§€ë‚œì£¼_df['unique_content'].explode())

    this_week_word_counts = Counter(this_week_words)
    last_week_word_counts = Counter(last_week_words)

    # ì´ë²ˆì£¼ì™€ ì§€ë‚œì£¼ì— ëª¨ë‘ ì–¸ê¸‰ëœ ë‹¨ì–´ë¥¼ ëª¨ì€ ì§‘í•©
    common_words = set(this_week_word_counts.keys()) & set(last_week_word_counts.keys())
    result = {}
    for word in common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ëª¨ë“  URLì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        url_list = list(ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: word in x)]['URL'])
        # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ URLì„ ì°¾ì•„ì„œ ì¶œë ¥
        url = max(url_list, key=lambda x: ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['URL'] == x, 'ì˜í–¥ë„'].iloc[0])
        increase_rate = (this_week_word_counts[word] - last_week_word_counts[word]) / this_week_word_counts[word]
        result[word] = {'ìƒìŠ¹ë¥ ': round(increase_rate, 2), 'URL': url}

    # ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
    keywords = []
    ups = []
    urls = []

    for word, data in sorted(result.items(), key=lambda x: x[1]['ìƒìŠ¹ë¥ '], reverse=True):
        if data['ìƒìŠ¹ë¥ ']>0:
            keywords.append(word)
            ups.append(f"{data['ìƒìŠ¹ë¥ ']*100}%")
            urls.append(data['URL'])

    result_df = pd.DataFrame({
        'ìƒìŠ¹ë¥ ': ups,
        'í‚¤ì›Œë“œ': keywords,
        'URL': urls
    })

    if len(result_df.index) >= 1 :
        return result_df

##í‚¤ì›Œë“œ##
st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
st.markdown(f"""<h3>ì‹ ê·œ í‚¤ì›Œë“œâ­ï¸</h3>""", unsafe_allow_html=True)
new_keyword = new_keyword(standard_df, new_df)
grouped_new_keyword = new_keyword.groupby('URL').agg({'í‚¤ì›Œë“œ': list, 'í‰ê·  ì˜í–¥ë„': 'first'}).reset_index()
grouped_new_keyword = grouped_new_keyword[['í‰ê·  ì˜í–¥ë„', 'í‚¤ì›Œë“œ', 'URL']].sort_values(by='í‰ê·  ì˜í–¥ë„', ascending=False)
st.dataframe(grouped_new_keyword)

st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
st.markdown(f"""<h3>ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œâ­ï¸</h3>""", unsafe_allow_html=True)
rising_keyword = rising_keyword(standard_df, new_df)
grouped_rising_keyword = rising_keyword.groupby('URL').agg({'í‚¤ì›Œë“œ': list, 'ìƒìŠ¹ë¥ ': 'first'}).reset_index()
grouped_rising_keyword = grouped_rising_keyword[['ìƒìŠ¹ë¥ ', 'í‚¤ì›Œë“œ', 'URL']].sort_values(by='ìƒìŠ¹ë¥ ', ascending=False)
st.dataframe(grouped_rising_keyword)

# except:
#     st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì‹ ê·œ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

# try:
#     rising_keyword = rising_keyword(standard_df, new_df)
# except:
#     st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

# ##ì‹ ê·œ í‚¤ì›Œë“œ##
# grouped_new_keyword = new_keyword.groupby('URL')
# key_counter = 1
# new_html_tags = ''
# for url, group in grouped_new_keyword:
#     keywords = ' | '.join(group['í‚¤ì›Œë“œ'])
#     percent = group['í‰ê·  ì˜í–¥ë„'].iloc[0]
#     key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
#     new_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ’«)</b>&nbsp;"

# ##ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ##    
# grouped_rising_keyword = rising_keyword.groupby('URL')
# key_counter = 1
# rising_html_tags = ''
# for url, group in grouped_rising_keyword:
#     keywords = ' | '.join(group['í‚¤ì›Œë“œ'])
#     percent = group['ìƒìŠ¹ë¥ '].iloc[0]
#     key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
#     rising_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ”¥)</b>&nbsp;"

# pd.DataFrame(new_keyword)
# pd.DataFrame(grouped_new_keyword)

# #HTML


# )
# st.markdown(f"""
#     <h3>ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œğŸ“ˆ</h3>

# )

