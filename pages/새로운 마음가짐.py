

#ì‹œê°í™”
import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import networkx as nx
from gensim.models import Word2Vec

import pandas as pd
import ast
import time
from datetime import datetime, timedelta
import itertools
from markdownlit import mdlit

#ìŠ¤íŠ¸ë¦¼ì‡
import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags
import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

#ê³„ì‚°
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud


#####Custom CSS styles#####
STYLE = """
.callout {
    padding: 1em;
    border-radius: 0.5em;
    background-color: #F8F8F8;
    border-left: 4px solid #f74040;
    margin-bottom: 1em;
    color: black;
}

.callout a#key1 {
    color: #000;
    background-color: #FAF3DD;
    text-decoration: none;
}

.callout a#key2 {
    color: #000;
    background-color: #E9F3F7;
    text-decoration: none;
}

.callout a#key3 {
    color: #000;
    background-color: #F6F3F8;
    text-decoration: none;
}

.callout a#key4 {
    color: #000;
    background-color: #EEF3ED;
    text-decoration: none;
}
"""

# rain(emoji="ğŸ¦",
#     font_size=54,
#     falling_speed=5,
#     animation_length="infinite")

######ë°ì´í„°#########
def to_list(text):
    return ast.literal_eval(text)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
# df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)

def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    
    return standard_df, new_df

######################ëŒ€ì‹œë³´ë“œ
st.title('ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ')

#### ì¸í’‹ í•„í„° #####
col1, col2, col3 = st.beta_columns(3)

min_date = datetime(2022, 6, 1)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2022,6,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2022,6,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=60))

with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'))

with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 80)
    effect_size = (100-int(temp_effect_size))/100

standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

#####ì›Œë“œ í´ë¼ìš°ë“œ########
##Countê¸°ì¤€###
def get_tfidf_top_words(df, keyword_no):
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    tfidf_top_words = dict(tfidf_top_words)
    return tfidf_top_words

def get_count_top_words(df, keyword_no):
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    return count_top_words

###ì‹œê°í™”####
expander = st.expander('ì›Œë“œ í´ë¼ìš°ë“œ ì„¸ë¶€í•„í„°')
with expander:
    col1, col2= st.beta_columns(2)    
    with col1:
        type = st.selectbox('ê¸°ì¤€',('ë‹¨ìˆœ ë¹ˆë„(Countvecterize)','ìƒëŒ€ ë¹ˆë„(TF-IDF)'))
    with col2:
        keyword_no = st.number_input("í‚¤ì›Œë“œ ë³¼ë¥¨", value=100, min_value=1, step=1)
   
    stopwords = st_tags(
        label = 'ì œê±°í•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”',
        value = ['ì‹ë¬¼', 'í™”ë¶„'],
        suggestions = ['ì‹ë¬¼', 'í™”ë¶„'],
        key = '1')
    # input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ')
    # stopwords = [x.strip() for x in input_str.split(',')]

try :
    if type == 'ë‹¨ìˆœ ë¹ˆë„(Countvecterize)' :
        words = get_count_top_words(standard_df, keyword_no)
    else :
        words = get_tfidf_top_words(standard_df, keyword_no)

    #ì›Œë“œí´ë¼ìš°ë“œ
    wc = WordCloud(background_color="white", colormap='Spectral', contour_color='steelblue', font_path="/app/streamlit/font/Pretendard-Bold.otf")
    wc.generate_from_frequencies(words)

    ###########ë™ì  ì›Œë“œ í´ë¼ìš°ë“œ####################
    # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„±
    word_list=[]
    freq_list=[]
    fontsize_list=[]
    position_list=[]
    orientation_list=[]
    color_list=[]

    for (word, freq), fontsize, position, orientation, color in wc.layout_:
        word_list.append(word)
        freq_list.append(freq)
        fontsize_list.append(fontsize)
        position_list.append(position)
        orientation_list.append(orientation)
        color_list.append(color)

    # get the positions
    x=[]
    y=[]
    for i in position_list:
        x.append(i[0])
        y.append(i[1])

    # WordCloud ì‹œê°í™”ë¥¼ ìœ„í•œ Scatter Plot ìƒì„±
    fig = go.Figure(go.Scatter(
        x=x, y=y, mode="text",
        text=word_list,
        textfont=dict(size=fontsize_list, color=color_list),
    ))
    fig.update_layout(xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False), hovermode='closest')
    st.plotly_chart(fig, use_container_width=True)

except :
    st.warning('ì˜í–¥ë„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”! ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ğŸ‘»')    


#### í‚¤ì›Œë“œ íë ˆì´íŒ… #####
### ì‹ ê·œ í‚¤ì›Œë“œ ###
def convert_to_markdown(row):
    return f"[`{row['í‚¤ì›Œë“œ']} | {row['í‰ê·  ì˜í–¥ë„']*100:.0f}`]({row['URL']})"

def make_keyword_tag(df):
    markdown_rows = df.apply(convert_to_markdown, axis=1).tolist()
    markdown_text = '   '.join(markdown_rows)
    return mdlit(f"""{markdown_text}""")

def new_keyword(standard_df, new_df):
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    content_list_1 = []
    content_list_1.extend(list(itertools.chain.from_iterable([eval(i) for i in standard_df['ì œëª©+ë‚´ìš©(nng)']])))
    content_list_2 = []
    content_list_2.extend(list(itertools.chain.from_iterable([eval(i) for i in new_df['ì œëª©+ë‚´ìš©(nng)']])))

    new_keywords = set(content_list_2) - set(content_list_1)   
    result_dict = {}
    # ì´ë²ˆë‹¬ì—ë§Œ ìˆëŠ” 
    for word in new_keywords:
        word_df = new_df[new_df['ì œëª©+ë‚´ìš©(nng)'].str.contains(word)]
        if len(word_df) > 0:
            avg_views = word_df['ì˜í–¥ë„'].mean()
            urls = word_df['URL'].tolist()
            result_dict[word] = {'í‰ê·  ì˜í–¥ë„': float(avg_views), 'URL': urls}
            
    # ì¡°íšŒìˆ˜ ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬        
    result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1]['í‰ê·  ì˜í–¥ë„'], reverse=True))    

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    keywords = []
    avg_views = []
    urls = []
    
    for key, value in result_dict.items():
        keywords.append(key)
        avg_views.append(value['í‰ê·  ì˜í–¥ë„'])
        urls.append('\n'.join(value['URL']))
    
    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'í‰ê·  ì˜í–¥ë„': avg_views,
        'URL': urls
    })

    return result_df

def rising_keyword(standard_df, new_df):
    # ë°ì´í„° í•©ì¹˜ê¸° 
    df = pd.concat([standard_df, new_df])

    # ë‚ ì§œ êµ¬í•˜ê¸°
    ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚  = df['ë‚ ì§œ'].max()
    ì´ë²ˆì£¼ì²«ë‚  = (df['ë‚ ì§œ'].max() - timedelta(days=7))
    ì§€ë‚œì£¼ì²«ë‚  = ì´ë²ˆì£¼ì²«ë‚  - timedelta(days=7)
    
    ì´ë²ˆì£¼_df = df[(df['ë‚ ì§œ'] > ì´ë²ˆì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚ )]
    ì§€ë‚œì£¼_df = df[(df['ë‚ ì§œ'] > ì§€ë‚œì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ì²«ë‚ )]
        
    # ì¤‘ë³µê°’ ì œê±°í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€
    ì´ë²ˆì£¼_df = ì´ë²ˆì£¼_df.copy()
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    ì§€ë‚œì£¼_df = ì§€ë‚œì£¼_df.copy()
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    this_week_words = list(ì´ë²ˆì£¼_df['unique_content'].explode())
    last_week_words = list(ì§€ë‚œì£¼_df['unique_content'].explode())

    this_week_word_counts = Counter(this_week_words)
    last_week_word_counts = Counter(last_week_words)

    # ì´ë²ˆì£¼ì™€ ì§€ë‚œì£¼ì— ëª¨ë‘ ì–¸ê¸‰ëœ ë‹¨ì–´ë¥¼ ëª¨ì€ ì§‘í•©
    common_words = set(this_week_word_counts.keys()) & set(last_week_word_counts.keys())
    result = {}
    for word in common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ëª¨ë“  URLì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        url_list = list(ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: word in x)]['URL'])
        # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ URLì„ ì°¾ì•„ì„œ ì¶œë ¥
        url = max(url_list, key=lambda x: ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['URL'] == x, 'ì˜í–¥ë„'].iloc[0])
        increase_rate = (this_week_word_counts[word] - last_week_word_counts[word]) / this_week_word_counts[word]
        result[word] = {'ìƒìŠ¹ë¥ ': round(increase_rate, 2), 'URL': url}

    # ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
    keywords = []
    ups = []
    urls = []

    for word, data in sorted(result.items(), key=lambda x: x[1]['ìƒìŠ¹ë¥ '], reverse=True):
        if data['ìƒìŠ¹ë¥ ']>0:
            keywords.append(word)
            ups.append(f"{data['ìƒìŠ¹ë¥ ']*100}%")
            urls.append(data['URL'])

    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'ìƒìŠ¹ë¥ ': ups,
        'URL': urls
    })

    if len(result_df.index) >= 1 :
        return result_df

### í‚¤ì›Œë“œ ###
try:
    new_keyword = new_keyword(standard_df, new_df)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì‹ ê·œ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

try:
    rising_keyword = rising_keyword(standard_df, new_df)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

##ì‹ ê·œ í‚¤ì›Œë“œ##
grouped_new_keyword = new_keyword.groupby('URL')
key_counter = 1
new_html_tags = ''
for url, group in grouped_new_keyword:
    keywords = ' | '.join(group['í‚¤ì›Œë“œ'])
    percent = group['í‰ê·  ì˜í–¥ë„'].iloc[0]
    key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
    new_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ’«)</b>&nbsp;"

##ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ##    
grouped_rising_keyword = rising_keyword.groupby('URL')
key_counter = 1
rising_html_tags = ''
for url, group in grouped_rising_keyword:
    keywords = ' '.join(group['í‚¤ì›Œë“œ'])
    percent = group['ìƒìŠ¹ë¥ '].iloc[0]
    key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
    rising_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ”¥)</b>&nbsp;"

#HTML
st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
st.markdown(f"""
    <h3>ì‹ ê·œ í‚¤ì›Œë“œâ­ï¸</h3>
    <div class='callout'>
    {new_html_tags}
    </div>""",
    unsafe_allow_html=True
)
st.markdown(f"""
    <h3>ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œğŸ“ˆ</h3>
    <div class='callout'>
    {rising_html_tags}
    </div>""",
    unsafe_allow_html=True
)

########### í‚¤ì›Œë“œ DeepDive ###########
df2 = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df2['ë‚ ì§œ'] = pd.to_datetime(df2['ë‚ ì§œ'])

st.title('ğŸ” í‚¤ì›Œë“œ DeepDive')
col1, col2 = st.beta_columns((0.2, 0.8))
keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='ì œë¼ëŠ„')
keyword2 = st_tags(
    label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
    text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
    value = ['ì‹ë¬¼ì˜ì–‘ì œ', 'ë¿Œë¦¬ì˜ì–‘ì œ'],
    maxtags = 5,
    key = '2')

def get_df(df, word1, args):
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
    result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
    keywords = [word1] + (args)
    result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
    for arg in keywords:
        if arg not in ' '.join(result['ì œëª©+ë‚´ìš©(nng)'].tolist()):
            st.warning(f"'ë‹¤ìŒ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. {arg}'")
            return None, None
    return result, keywords

def deepdive_lineplot(df, keywords):
    # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    keywords = keywords[::-1]
    keyword_dfs = {}
    for keyword in keywords:
        keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    impact_by_week = {}
    for keyword, keyword_df in keyword_dfs.items():
        keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
        keyword_df.set_index('ë‚ ì§œ', inplace=True)
        impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

    # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    for i, (keyword, impact) in enumerate(impact_by_week.items()):
        fig.add_trace(go.Scatter(x=impact.index, y=impact.values, name=keyword, line_color=colors[i]), secondary_y=False)
        
    fig.update_layout(title_text="ì‹œê°„ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„", yaxis_title="í‰ê·  ì˜í–¥ë„")
    st.plotly_chart(fig, use_container_width=True)

def get_TOP_10(df, keyword):
    temp_df = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)]
    top10_list = []
    for media_category in temp_df['ë§¤ì²´'].unique():
        df_category = temp_df[temp_df['ë§¤ì²´'] == media_category]
        if len(df_category) > 0:
            try:
                band_top10 = df_category.nlargest(10, 'ì˜í–¥ë„')
                band_top10['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                band_top10 = band_top10.reset_index(drop=True)
                band_top10 = band_top10[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(band_top10)
            except ValueError:
                df_category['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                df_category = df_category.reset_index(drop=True)
                df_category = df_category[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(df_category)
    if len(top10_list) > 0:
        return pd.concat(top10_list, ignore_index=False)
    else:
        return None
    
try :
    deepdive_df, deepdive_keywords = get_df(df2, keyword1, keyword2)
    deepdive_lineplot(deepdive_df, deepdive_keywords)
    keyword_result = get_TOP_10(df2, keyword1)
    st.dataframe(keyword_result)

except :
    st.warning("í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

# col1, col2 = st.beta_columns((0.2, 0.8))
# deep_keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='í•´ì¶©ì œ')
# deep_keyword2 = st_tags(
#     label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
#     text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
#     value = ['ì‹ë¬¼ì˜ì–‘ì œ', 'ë¿Œë¦¬ì˜ì–‘ì œ'],
#     suggestions = ['í•´ì¶©ì œ', 'ì œë¼ëŠ„'],
#     maxtags = 5,
#     key = '1')

# def get_df(df, word1, args):
#     # word1 ì€ ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•˜ëŠ” ê¸°ì¤€
#     # ì…ë ¥í•œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ ì´ìƒì´ í¬í•¨ëœ í–‰ ì°¾ê¸°
#     df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
#     result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
#     result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
#     keywords = [word1] + (args)
#     result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
#     return result

# def plot_keyword_impact_grey(df, keywords):
#     # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    
#     keywords = keywords[::-1]
#     keyword_dfs = {}
#     for keyword in keywords:
#         keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()
    
#     # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
#     impact_by_week = {}
#     for keyword, keyword_df in keyword_dfs.items():
#         keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
#         keyword_df.set_index('ë‚ ì§œ', inplace=True)
#         impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

#     # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
#     fig = make_subplots(specs=[[{"secondary_y": True}]])
    
#     # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
#     colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    
#     for i, (keyword, impact) in enumerate(impact_by_week.items()):
#         fig.add_trace(go.Scatter(x=impact.index, y=impact.values, name=keyword, line_color=colors[i]), secondary_y=False)
        
#     fig.update_layout(title_text="ì‹œê°„ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„", xaxis_title="ë‚ ì§œ", yaxis_title="í‰ê·  ì˜í–¥ë„")
#     st.plotly_chart(fig, use_container_width=True)

# hello = get_df(df, deep_keyword1, deep_keyword2)
# hello
# # plot_keyword_impact_grey(deepdive_df, keyword_list)
