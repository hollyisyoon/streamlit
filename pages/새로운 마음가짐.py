import pandas as pd
import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import ast
import time

import streamlit as st
from streamlit_extras.let_it_rain import rain

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
from datetime import datetime, timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")
import networkx as nx
from gensim.models import Word2Vec
import time
import itertools

rain(emoji="ğŸ¦",
    font_size=54,
    falling_speed=10,
    animation_length="infinite")

######ë°ì´í„°#########
def to_list(text):
    return ast.literal_eval(text)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
# df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)

def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    
    return standard_df, new_df

######################ëŒ€ì‹œë³´ë“œ
st.title('ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ')

#### ì¸í’‹ í•„í„° #####
col1, col2, col3 = st.beta_columns(3)

min_date = datetime(2022, 7, 25)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2023,4,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2023,4,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=90))

with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'))

with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 30)
    effect_size = (100-int(temp_effect_size))/100

standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

#####ì›Œë“œ í´ë¼ìš°ë“œ########
##Countê¸°ì¤€###
def get_tfidf_top_words(df, keyword_no):
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    tfidf_top_words = dict(tfidf_top_words)
    return tfidf_top_words

def get_count_top_words(df, keyword_no):
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    return count_top_words

###ì‹œê°í™”####
col1, col2 = st.beta_columns((0.2, 0.8))
with col1:
    type = st.selectbox('ê¸°ì¤€',('ë‹¨ìˆœ ë¹ˆë„(Countvecterize)','ìƒëŒ€ ë¹ˆë„(TF-IDF)'))
    keyword_no = st.number_input("í‚¤ì›Œë“œ ë³¼ë¥¨", value=100, min_value=1, step=1)
    input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ')
    stopwords = [x.strip() for x in input_str.split(',')]

with col2:
    try :
        if type == 'ë‹¨ìˆœ ë¹ˆë„(Countvecterize)' :
            words = get_count_top_words(standard_df, keyword_no)
        else :
            words = get_tfidf_top_words(standard_df, keyword_no)

        #ì›Œë“œí´ë¼ìš°ë“œ
        wc = WordCloud(background_color="white", colormap='Spectral', contour_color='steelblue', font_path="/app/streamlit/font/Pretendard-Bold.otf")
        wc.generate_from_frequencies(words)

        ###########ë™ì  ì›Œë“œ í´ë¼ìš°ë“œ####################
        # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„±
        word_list=[]
        freq_list=[]
        fontsize_list=[]
        position_list=[]
        orientation_list=[]
        color_list=[]

        for (word, freq), fontsize, position, orientation, color in wc.layout_:
            word_list.append(word)
            freq_list.append(freq)
            fontsize_list.append(fontsize)
            position_list.append(position)
            orientation_list.append(orientation)
            color_list.append(color)

        # get the positions
        x=[]
        y=[]
        for i in position_list:
            x.append(i[0])
            y.append(i[1])

        # WordCloud ì‹œê°í™”ë¥¼ ìœ„í•œ Scatter Plot ìƒì„±
        fig = go.Figure(go.Scatter(
            x=x, y=y, mode="text",
            text=word_list,
            textfont=dict(size=fontsize_list, color=color_list),
        ))
        fig.update_layout(xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False), hovermode='closest')
        st.plotly_chart(fig, use_container_width=True)

    except :
        st.warning('ì˜í–¥ë„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”! ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ğŸ‘»')    


#### í‚¤ì›Œë“œ íë ˆì´íŒ… #####
### ì‹ ê·œ í‚¤ì›Œë“œ ###
def convert_to_markdown(row):
    return f"[`{row['í‚¤ì›Œë“œ']} | {row['í‰ê·  ì˜í–¥ë„']:.6f}`]({row['URL']})"

def make_keyword_tag(df):
    markdown_rows = df.apply(convert_to_markdown, axis=1).tolist()
    markdown_text = '   '.join(markdown_rows)
    return mdlit(f"""{markdown_text}""")

def new_keyword(standard_df, new_df):
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    content_list_1 = []
    content_list_1.extend(list(itertools.chain.from_iterable([eval(i) for i in standard_df['ì œëª©+ë‚´ìš©(nng)']])))
    content_list_2 = []
    content_list_2.extend(list(itertools.chain.from_iterable([eval(i) for i in new_df['ì œëª©+ë‚´ìš©(nng)']])))

    new_keywords = set(content_list_2) - set(content_list_1)   
    result_dict = {}
    # ì´ë²ˆë‹¬ì—ë§Œ ìˆëŠ” 
    for word in new_keywords:
        word_df = new_df[new_df['ì œëª©+ë‚´ìš©(nng)'].str.contains(word)]
        if len(word_df) > 0:
            avg_views = word_df['ì˜í–¥ë„'].mean()
            urls = word_df['URL'].tolist()
            result_dict[word] = {'í‰ê·  ì˜í–¥ë„': float(avg_views), 'URL': urls}
            
    # ì¡°íšŒìˆ˜ ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬        
    result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1]['í‰ê·  ì˜í–¥ë„'], reverse=True))    

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    keywords = []
    avg_views = []
    urls = []
    
    for key, value in result_dict.items():
        keywords.append(key)
        avg_views.append(value['í‰ê·  ì˜í–¥ë„'])
        urls.append('\n'.join(value['URL']))
    
    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'í‰ê·  ì˜í–¥ë„': avg_views,
        'URL': urls
    })
    
    return result_df

new_keyword = new_keyword(standard_df, new_df)
make_keyword_tag(new_keyword)
new_keyword
standard_df, new_df
