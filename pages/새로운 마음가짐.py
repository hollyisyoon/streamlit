import pandas as pd

#ì‹œê°í™”
import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import ast
import time

import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
from datetime import datetime, timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")
import networkx as nx
from gensim.models import Word2Vec
import time
import itertools
from markdownlit import mdlit

rain(emoji="ğŸ¦",
    font_size=54,
    falling_speed=5,
    animation_length="infinite")

######ë°ì´í„°#########
def to_list(text):
    return ast.literal_eval(text)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
# df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)

def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    
    return standard_df, new_df

######################ëŒ€ì‹œë³´ë“œ
st.title('ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ')

#### ì¸í’‹ í•„í„° #####
col1, col2, col3 = st.beta_columns(3)

min_date = datetime(2022, 6, 1)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2022,6,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2022,6,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=60))

with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'))

with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 80)
    effect_size = (100-int(temp_effect_size))/100

standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

#####ì›Œë“œ í´ë¼ìš°ë“œ########
##Countê¸°ì¤€###
def get_tfidf_top_words(df, keyword_no):
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    tfidf_top_words = dict(tfidf_top_words)
    return tfidf_top_words

def get_count_top_words(df, keyword_no):
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    return count_top_words

###ì‹œê°í™”####
col1, col2 = st.beta_columns((0.2, 0.8))
with col1:
    type = st.selectbox('ê¸°ì¤€',('ë‹¨ìˆœ ë¹ˆë„(Countvecterize)','ìƒëŒ€ ë¹ˆë„(TF-IDF)'))
    keyword_no = st.number_input("í‚¤ì›Œë“œ ë³¼ë¥¨", value=100, min_value=1, step=1)
    input_str = st.text_input('ì œê±°í•  í‚¤ì›Œë“œ')
    stopwords = [x.strip() for x in input_str.split(',')]

with col2:
    try :
        if type == 'ë‹¨ìˆœ ë¹ˆë„(Countvecterize)' :
            words = get_count_top_words(standard_df, keyword_no)
        else :
            words = get_tfidf_top_words(standard_df, keyword_no)

        #ì›Œë“œí´ë¼ìš°ë“œ
        wc = WordCloud(background_color="white", colormap='Spectral', contour_color='steelblue', font_path="/app/streamlit/font/Pretendard-Bold.otf")
        wc.generate_from_frequencies(words)

        ###########ë™ì  ì›Œë“œ í´ë¼ìš°ë“œ####################
        # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„±
        word_list=[]
        freq_list=[]
        fontsize_list=[]
        position_list=[]
        orientation_list=[]
        color_list=[]

        for (word, freq), fontsize, position, orientation, color in wc.layout_:
            word_list.append(word)
            freq_list.append(freq)
            fontsize_list.append(fontsize)
            position_list.append(position)
            orientation_list.append(orientation)
            color_list.append(color)

        # get the positions
        x=[]
        y=[]
        for i in position_list:
            x.append(i[0])
            y.append(i[1])

        # WordCloud ì‹œê°í™”ë¥¼ ìœ„í•œ Scatter Plot ìƒì„±
        fig = go.Figure(go.Scatter(
            x=x, y=y, mode="text",
            text=word_list,
            textfont=dict(size=fontsize_list, color=color_list),
        ))
        fig.update_layout(xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False), hovermode='closest')
        st.plotly_chart(fig, use_container_width=True)

    except :
        st.warning('ì˜í–¥ë„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”! ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ğŸ‘»')    


#### í‚¤ì›Œë“œ íë ˆì´íŒ… #####
### ì‹ ê·œ í‚¤ì›Œë“œ ###
def convert_to_markdown(row):
    return f"[`{row['í‚¤ì›Œë“œ']} | {row['í‰ê·  ì˜í–¥ë„']*100:.0f}`]({row['URL']})"

def convert_to_markdown2(row):
    return f"[`{row['í‚¤ì›Œë“œ']} | {row['ìƒìŠ¹ë¥ ']*100:.0f}`]({row['URL']})"

def make_keyword_tag(df):
    markdown_rows = df.apply(convert_to_markdown, axis=1).tolist()
    markdown_text = '   '.join(markdown_rows)
    return mdlit(f"""{markdown_text}""")

def make_keyword_tag2(df):
    markdown_rows = df.apply(convert_to_markdown2, axis=1).tolist()
    markdown_text = '   '.join(markdown_rows)
    return mdlit(f"""{markdown_text}""")

def new_keyword(standard_df, new_df):
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    content_list_1 = []
    content_list_1.extend(list(itertools.chain.from_iterable([eval(i) for i in standard_df['ì œëª©+ë‚´ìš©(nng)']])))
    content_list_2 = []
    content_list_2.extend(list(itertools.chain.from_iterable([eval(i) for i in new_df['ì œëª©+ë‚´ìš©(nng)']])))

    new_keywords = set(content_list_2) - set(content_list_1)   
    result_dict = {}
    # ì´ë²ˆë‹¬ì—ë§Œ ìˆëŠ” 
    for word in new_keywords:
        word_df = new_df[new_df['ì œëª©+ë‚´ìš©(nng)'].str.contains(word)]
        if len(word_df) > 0:
            avg_views = word_df['ì˜í–¥ë„'].mean()
            urls = word_df['URL'].tolist()
            result_dict[word] = {'í‰ê·  ì˜í–¥ë„': float(avg_views), 'URL': urls}
            
    # ì¡°íšŒìˆ˜ ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬        
    result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1]['í‰ê·  ì˜í–¥ë„'], reverse=True))    

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    keywords = []
    avg_views = []
    urls = []
    
    for key, value in result_dict.items():
        keywords.append(key)
        avg_views.append(value['í‰ê·  ì˜í–¥ë„'])
        urls.append('\n'.join(value['URL']))
    
    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'í‰ê·  ì˜í–¥ë„': avg_views,
        'URL': urls
    })

    return result_df[:20]

def rising_keyword(standard_df, new_df):
    # ë°ì´í„° í•©ì¹˜ê¸° 
    df = pd.concat([standard_df, new_df])
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    
    # ë‚ ì§œ êµ¬í•˜ê¸°
    ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚  = df['ë‚ ì§œ'].max()
    ì´ë²ˆì£¼ì²«ë‚  = (df['ë‚ ì§œ'].max() - timedelta(days=7))
    ì§€ë‚œì£¼ì²«ë‚  = ì´ë²ˆì£¼ì²«ë‚  - timedelta(days=7)
    
    ì´ë²ˆì£¼_df = df[(df['ë‚ ì§œ'] > ì´ë²ˆì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚ )]
    ì§€ë‚œì£¼_df = df[(df['ë‚ ì§œ'] > ì§€ë‚œì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ì²«ë‚ )]
        
    # ì¤‘ë³µê°’ ì œê±°í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€
    ì´ë²ˆì£¼_df = ì´ë²ˆì£¼_df.copy()
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    ì§€ë‚œì£¼_df = ì§€ë‚œì£¼_df.copy()
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    this_week_words = list(ì´ë²ˆì£¼_df['unique_content'].explode())
    last_week_words = list(ì§€ë‚œì£¼_df['unique_content'].explode())

    this_week_word_counts = Counter(this_week_words)
    last_week_word_counts = Counter(last_week_words)

    # ì´ë²ˆì£¼ì™€ ì§€ë‚œì£¼ì— ëª¨ë‘ ì–¸ê¸‰ëœ ë‹¨ì–´ë¥¼ ëª¨ì€ ì§‘í•©
    common_words = set(this_week_word_counts.keys()) & set(last_week_word_counts.keys())
    result = {}
    for word in common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ëª¨ë“  URLì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        url_list = list(ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: word in x)]['URL'])
        # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ URLì„ ì°¾ì•„ì„œ ì¶œë ¥
        url = max(url_list, key=lambda x: ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['URL'] == x, 'ì˜í–¥ë„'].iloc[0])
        increase_rate = (this_week_word_counts[word] - last_week_word_counts[word]) / this_week_word_counts[word]
        result[word] = {'ìƒìŠ¹ë¥ ': round(increase_rate, 2), 'URL': url}
    
    # ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
    keywords = []
    ups = []
    urls = []

    for word, data in sorted(result.items(), key=lambda x: x[1]['ìƒìŠ¹ë¥ '], reverse=True):
        if data['ìƒìŠ¹ë¥ ']>0:
            keywords.append(word)
            ups.append(f"{data['ìƒìŠ¹ë¥ ']}%")
            urls.append(data['URL'])

    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'ìƒìŠ¹ë¥ ': ups,
        'URL': urls
    })

    if len(result_df.index) >= 1 :
        return result_df
    
### í‚¤ì›Œë“œ ###
st.subheader('âœ¨ ì‹ ê·œ í‚¤ì›Œë“œ')
try:
    new_keyword = new_keyword(standard_df, new_df)
    make_keyword_tag(new_keyword)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì‹ ê·œ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

st.subheader('ğŸ”¥ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ')
try:
    rising_keyword = rising_keyword(standard_df, new_df)
    make_keyword_tag2(rising_keyword)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

########### í‚¤ì›Œë“œ DeepDive ###########
st.title('ğŸ” í‚¤ì›Œë“œ DeepDive')
col1, col2 = st.beta_columns((0.2, 0.8))
keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='í•´ì¶©ì œ')
keyword2 = st_tags(
    label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
    text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
    value = ['ì‹ë¬¼ì˜ì–‘ì œ', 'ë¿Œë¦¬ì˜ì–‘ì œ'],
    suggestions = ['í•´ì¶©ì œ', 'ì œë¼ëŠ„'],
    maxtags = 5,
    key = '1')

def get_df(df, word1, *args):
    # word1 ì€ ë°˜ë“œì‹œ ì…ë ¥í•´ì•¼ í•˜ëŠ” ê¸°ì¤€
    # ì…ë ¥í•œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ ì´ìƒì´ í¬í•¨ëœ í–‰ ì°¾ê¸°
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
    result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
    keywords = [word1] + list(args)

    return keywords
    # result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
    
    # # ì…ë ¥í•œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
    # for arg in keywords:
    #     if arg not in ' '.join(result['ì œëª©+ë‚´ìš©(nng)'].tolist()):
    #         return f"'{arg}'ëŠ” í•œ ë²ˆë„ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    # return result, keywords

def plot_keyword_impact_grey(df, keywords):
    # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    
    keywords = keywords[::-1]
    keyword_dfs = {}
    for keyword in keywords:
        keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    impact_by_week = {}
    for keyword, keyword_df in keyword_dfs.items():
        keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
        keyword_df.set_index('ë‚ ì§œ', inplace=True)
        impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

    # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    
    for i, (keyword, impact) in enumerate(impact_by_week.items()):
        fig.add_trace(go.Scatter(x=impact.index, y=impact.values, name=keyword, line_color=colors[i]), secondary_y=False)
        
    fig.update_layout(title_text="ì‹œê°„ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„", xaxis_title="ë‚ ì§œ", yaxis_title="í‰ê·  ì˜í–¥ë„")
    st.plotly_chart(fig, use_container_width=True)

keyword2
get_df(df, keyword1, keyword2)
# plot_keyword_impact_grey(deepdive_df, keyword_list)
