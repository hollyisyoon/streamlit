import streamlit as st
import streamlit.components.v1 as components
from streamlit_tags import st_tags

import plotly.express as px
import plotly.graph_objects as go
import plotly.subplots as sp

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import ast
from datetime import datetime
from datetime import timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import koreanize_matplotlib

from PIL import Image

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from gensim.models import Word2Vec
import networkx as nx
import gensim
from pyvis.network import Network
from wordcloud import WordCloud

css_code = """
<style>
    .custom-sidebar {
        padding: 20px;
        background-color: #f2f2f2;
        font-size: 18px;
        color: #333;
    }
    
    .custom-sidebar a {
        color: #333;
        text-decoration: none;
    }
</style>
"""

STYLE = """
.callout {
    padding: 1em;
    border-radius: 0.5em;
    background-color: #F8F8F8;
    border-left: 4px solid #195ef7;
    margin-bottom: 1em;
    color: black;
}

.callout a#key1 {
    color: #000;
    background-color: #FAF3DD;
    text-decoration: none;
}

.callout a#key2 {
    color: #000;
    background-color: #E9F3F7;
    text-decoration: none;
}

.callout a#key3 {
    color: #000;
    background-color: #F6F3F8;
    text-decoration: none;
}

.callout a#key4 {
    color: #000;
    background-color: #EEF3ED;
    text-decoration: none;
}
"""

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
# st.title('ğŸ” íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶„ì„')

#########Section3 - í‚¤ì›Œë“œ deepdive(ì‹œê³„ì—´)############
st.markdown("<h2 id='section4'>í‚¤ì›Œë“œ ì‹œê³„ì—´ ë¶„ì„</h2>", unsafe_allow_html=True)
col1, col2 = st.beta_columns((0.3, 0.7))
with col1:
    keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='ì œë¼ëŠ„')
with col2:
    st.write(' ')

keyword2 = st_tags(
        label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
        value = ['ì´ì±„ë²Œë ˆ','ë¿Œë¦¬íŒŒë¦¬'],
        maxtags = 5,
        key = '2')

def get_df(df, word1, args):
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
    result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
    keywords = [word1] + (args)
    result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
    for arg in keywords:
        if arg not in ' '.join(result['ì œëª©+ë‚´ìš©(nng)'].tolist()):
            st.warning(f"'ë‹¤ìŒ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. {arg}'")
            return None, None
    return result, keywords

def deepdive_lineplot(df, keywords):
    # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    keywords = keywords[::-1]
    keyword_dfs = {}
    for keyword in keywords:
        keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()

    # ì „ì²´ ê¸°ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ì°¨ ë‹¨ìœ„ë¡œ resampling í•˜ê¸° ìœ„í•´
    # ë‚ ì§œ ë²”ìœ„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    date_range = pd.date_range(start=df['ë‚ ì§œ'].min(), end=df['ë‚ ì§œ'].max(), freq='W')

    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    impact_by_week = {}
    for keyword, keyword_df in keyword_dfs.items():
        keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
        keyword_df.set_index('ë‚ ì§œ', inplace=True)
        impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

    # ê° í‚¤ì›Œë“œë³„ë¡œ ì¸ë±ìŠ¤ë¥¼ ë§ì¶”ì–´ì£¼ê³ , ë°ì´í„°ê°€ ì—†ëŠ” ë¶€ë¶„ì€ ë³´ê°„í•˜ì—¬ ì±„ì›Œì¤ë‹ˆë‹¤.
    for keyword in keywords:
        impact = impact_by_week[keyword].reindex(date_range, fill_value=pd.NaT)
        impact_by_week[keyword] = impact.interpolate()

    # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    fig = sp.make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    for i, (keyword, impact) in enumerate(impact_by_week.items()):
        # ë³´ê°„ëœ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        interpolated_idx = impact[impact.isna()].index
        # ì˜í–¥ë„ ë°ì´í„°ë¥¼ ë³´ê°„í•©ë‹ˆë‹¤.
        impact = impact.interpolate()
        fig.add_trace(
            go.Scatter(
                x=impact.index,
                y=impact.values,
                name=keyword,
                line_color=colors[i]
               
            ),
            secondary_y=False
        )
        
    fig.update_layout(yaxis_title="í‰ê·  ì˜í–¥ë„")
    st.plotly_chart(fig, use_container_width=True)
try:
    deepdive_df, deepdive_keywords = get_df(df, keyword1, keyword2)
    deepdive_lineplot(deepdive_df, deepdive_keywords)
except :
    st.warning("í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

#########Section4 - í‚¤ì›Œë“œ deepdive(ìƒìœ„ ê²Œì‹œê¸€)############
import re

def get_TOP_post(df, media, deepdive_keywords):
    df = df[df['ë§¤ì²´'] == media]
    df['ì˜í–¥ë„'] *= 100 

    top_list = []
    for deepdive_keyword in deepdive_keywords:
        keyword_df = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(deepdive_keyword)]
        keyword_df['í‚¤ì›Œë“œ'] = deepdive_keyword
        if len(keyword_df) > 0:
            keyword_df = keyword_df.nlargest(min(len(keyword_df), 10), 'ì˜í–¥ë„')
            top_list.append(keyword_df)
    
    if top_list:
        top_df = pd.concat(top_list)
        top_df = top_df[['í‚¤ì›Œë“œ', 'ì˜í–¥ë„', 'ì‘ì„±ì', 'ì œëª©', 'URL']]
        top_df.sort_values(by=['ì˜í–¥ë„'], ascending=[False], inplace=True)
        top_df = top_df.reset_index(drop=True)
        return top_df
    else:
        return None

def get_Top10_writer(df, media, deepdive_keywords):
    df = df[df['ë§¤ì²´'] == media]
    df['ì˜í–¥ë„'] *= 100 

    writer_scores = {}
    for deepdive_keyword in deepdive_keywords:
        keyword_df = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(deepdive_keyword)]
        if len(keyword_df) > 0:
            grouped = keyword_df.groupby('ì‘ì„±ì')['ì˜í–¥ë„'].mean()
            writer_scores.update(grouped)

    if len(writer_scores) == 0:
        return None

    top_writers = sorted(writer_scores.items(), key=lambda x: x[1], reverse=True)[:20]
    writer_names, scores = zip(*top_writers)

    urls = []
    hover_text = []
    for writer_name in writer_names:
        url = df[df['ì‘ì„±ì'] == writer_name]['URL'].iloc[0]
        urls.append(url)
        hover_text.append(f'ì‘ì„±ì: {writer_name}<br>URL: {url}')

    truncated_writer_names = [name[:7] + '...' if len(name) > 7 else name for name in writer_names]
    hover_text = [f'{name} ({url})' for name, url in zip(writer_names, urls)]

    fig = px.bar(x=truncated_writer_names, y=scores,
                title='ìƒìœ„ 20ìœ„ ì‘ì„±ìì˜ í‰ê·  ì˜í–¥ë„', 
                hover_data={'URL': urls, 'hover_text': hover_text})

    fig.update_layout(xaxis_tickangle=-45, yaxis_title='í‰ê·  ì˜í–¥ë„', xaxis_visible=False)

    return fig

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ì‹ë¬¼ê°¤ëŸ¬ë¦¬", "ì‹ë¬¼ë³‘ì›", "ë„¤ì´ë²„ì¹´í˜", "ë„¤ì´ë²„ë¸”ë¡œê·¸", "ë„¤ì´ë²„í¬ìŠ¤íŠ¸"])

with tab1:
    top_ì‹ë¬¼ê°¤ëŸ¬ë¦¬ = get_TOP_post(df, "ì‹ë¬¼ê°¤ëŸ¬ë¦¬", deepdive_keywords)
    if top_ì‹ë¬¼ê°¤ëŸ¬ë¦¬ is not None:
        st.dataframe(top_ì‹ë¬¼ê°¤ëŸ¬ë¦¬)
    else:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ì‹ë¬¼ê°¤ëŸ¬ë¦¬ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
with tab2:
    try:
        fig2 = get_Top10_writer(df, "ì‹ë¬¼ë³‘ì›", deepdive_keywords)
        st.plotly_chart(fig2)
    except:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ì‹ë¬¼ë³‘ì› ì‘ì„±ìê°€ ì—†ìŠµë‹ˆë‹¤")
    top_ì‹ë¬¼ë³‘ì› = get_TOP_post(df, "ì‹ë¬¼ë³‘ì›", deepdive_keywords)
    if top_ì‹ë¬¼ë³‘ì› is not None:
        st.dataframe(top_ì‹ë¬¼ë³‘ì›)
    else:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ì‹ë¬¼ë³‘ì› ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    try:
        fig3 = get_Top10_writer(df, "ë„¤ì´ë²„ì¹´í˜", deepdive_keywords)
        st.plotly_chart(fig3)
    except:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ì¹´í˜ ì‘ì„±ìê°€ ì—†ìŠµë‹ˆë‹¤")
    top_ë„¤ì´ë²„ì¹´í˜ = get_TOP_post(df, "ë„¤ì´ë²„ì¹´í˜", deepdive_keywords)
    if top_ë„¤ì´ë²„ì¹´í˜ is not None:
        st.dataframe(top_ë„¤ì´ë²„ì¹´í˜)
    else:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ì¹´í˜ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

with tab4:
    try:
        fig4 = get_Top10_writer(df, "ë„¤ì´ë²„ë¸”ë¡œê·¸", deepdive_keywords)
        st.plotly_chart(fig4)
    except:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ë¸”ë¡œê·¸ ì‘ì„±ìê°€ ì—†ìŠµë‹ˆë‹¤")
    top_ë„¤ì´ë²„ë¸”ë¡œê·¸ = get_TOP_post(df, "ë„¤ì´ë²„ë¸”ë¡œê·¸", deepdive_keywords)
    if top_ë„¤ì´ë²„ë¸”ë¡œê·¸ is not None:
        st.dataframe(top_ë„¤ì´ë²„ë¸”ë¡œê·¸)
    else:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")

with tab5:
    try:
        fig5 = get_Top10_writer(df, "ë„¤ì´ë²„í¬ìŠ¤íŠ¸", deepdive_keywords)
        st.plotly_chart(fig5)
    except:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„í¬ìŠ¤íŠ¸ ì‘ì„±ìê°€ ì—†ìŠµë‹ˆë‹¤")
    top_ë„¤ì´ë²„í¬ìŠ¤íŠ¸ = get_TOP_post(df, "ë„¤ì´ë²„í¬ìŠ¤íŠ¸", deepdive_keywords)
    if top_ë„¤ì´ë²„í¬ìŠ¤íŠ¸ is not None:
        st.dataframe(top_ë„¤ì´ë²„í¬ìŠ¤íŠ¸)
    else:
        st.warning("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë„¤ì´ë²„í¬ìŠ¤íŠ¸ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")


#########Section5 - í‚¤ì›Œë“œ deepdive(ë„¤íŠ¸ì›Œí¬ ë¶„ì„)############
st.markdown("---")
st.markdown("<h2 id='section4'>í‚¤ì›Œë“œ ì—°ê´€ë¶„ì„</h2>", unsafe_allow_html=True)

all_keywords = [keyword1]+keyword2
st.text(f'ğŸ”® {all_keywords}ì— ëŒ€í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤')

expander = st.expander('ì—°ê´€ë¶„ì„ ì„¸ë¶€í•„í„°')
with expander:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'), help="í™•ì¸í•˜ê³  ì‹¶ì€ ì™¸ë¶€ ë°ì´í„°ì˜ ë§¤ì²´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def extract_df(df, media):
    standard_df = df[(df['ë§¤ì²´'] == media)]
    return standard_df

df2 = extract_df(df, media)
network_list = [eval(i) for i in df2['ì œëª©+ë‚´ìš©(nng)']]

def ë„¤íŠ¸ì›Œí¬(network_list, all_keywords):
    networks = []
    for review in network_list:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=50)

    G = nx.Graph(font_path='/app/streamlit/font/Pretendard-Bold.otf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in all_keywords:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=15)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in all_keywords:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", edge_color="grey", width=edge_weights)

    net = Network(notebook=True, cdn_resources='in_line')
    net.from_nx(G)
    return [net, similar_words]

ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(network_list, all_keywords)
if st.button('ë¶„ì„ ì‹œì‘'):
    with st.spinner('ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        try:
            net = ë„¤íŠ¸ì›Œí¬[0]
            net.save_graph(f'/app/streamlit/pyvis_graph.html')
            HtmlFile = open(f'/app/streamlit/pyvis_graph.html', 'r', encoding='utf-8')
            components.html(HtmlFile.read(), height=600)
        except:
            st.warning('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')