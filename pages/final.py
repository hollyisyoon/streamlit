import koreanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from matplotlib.colors import to_rgba
import plotly.graph_objects as go
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pyvis.network import Network
import networkx as nx
import gensim
from gensim.models import Word2Vec
from PIL import Image

import pandas as pd
import ast
import time
from datetime import datetime, timedelta
import itertools
from markdownlit import mdlit

#ìŠ¤íŠ¸ë¦¼ì‡
import streamlit as st
from streamlit_extras.let_it_rain import rain
from streamlit_tags import st_tags
import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

#ê³„ì‚°
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud


# CSS ìŠ¤íƒ€ì¼ ì •ì˜
css_code = """
<style>
    .custom-sidebar {
        padding: 20px;
        background-color: #f2f2f2;
        font-size: 18px;
        color: #333;
    }
    
    .custom-sidebar a {
        color: #333;
        text-decoration: none;
    }
</style>
"""

STYLE = """
.callout {
    padding: 1em;
    border-radius: 0.5em;
    background-color: #F8F8F8;
    border-left: 4px solid #195ef7;
    margin-bottom: 1em;
    color: black;
}

.callout a#key1 {
    color: #000;
    background-color: #FAF3DD;
    text-decoration: none;
}

.callout a#key2 {
    color: #000;
    background-color: #E9F3F7;
    text-decoration: none;
}

.callout a#key3 {
    color: #000;
    background-color: #F6F3F8;
    text-decoration: none;
}

.callout a#key4 {
    color: #000;
    background-color: #EEF3ED;
    text-decoration: none;
}
"""

############## ì‚¬ì´ë“œë°”
# st.sidebar.markdown(f"<style>{css_code}</style>", unsafe_allow_html=True)
# st.sidebar.markdown("""
#     <div class="custom-sidebar">
#         <h2><a href="#section1">ğŸª„ í‚¤ì›ŒíŠ¸ ë°œêµ´</a></h2>
#         <h2><a href="#section2">ğŸ’ í‚¤ì›Œë“œ íë ˆì´ì…˜</a></h2>
#         <h2><a href="#section3">â³ ì‹œê¸°ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„</a></h2>
#         <h2><a href="#section4">ì„œë¸Œíƒ€ì´í‹€ 4</a></h2>
#     </div>
# """, unsafe_allow_html=True)

##############ë©”ì¸ ì½˜í…ì¸ 
st.title("ì™¸ë¶€ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")

#########Section1 - wordcloud############
st.markdown("<h2 id='section1'>ğŸª„ í‚¤ì›ŒíŠ¸ ë°œêµ´</h2>", unsafe_allow_html=True)

##ë°ì´í„°##
def to_list(text):
    return ast.literal_eval(text)

df = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
def extract_df(df, media, start_date, end_date, effect_size):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    standard_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date) & (df['ì˜í–¥ë„'] >= effect_size)]
    range_days = (end_date - start_date) + timedelta(days = 1)
    new_day = start_date - range_days
    new_day = pd.Timestamp(new_day)
    new_df = df[(df['ë§¤ì²´'] == media) & (df['ë‚ ì§œ'] >= new_day) & (df['ë‚ ì§œ'] < start_date) & (df['ì˜í–¥ë„'] >= effect_size)]    
    return standard_df, new_df

##ì¸í’‹ í•„í„° - ì „ì²´ìš©##
col1, col2, col3 = st.beta_columns(3)
min_date = datetime(2022, 6, 1)
max_date = datetime(2023, 4, 26)
with col1:
    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ",
                               value=datetime(2022,6,1),
                               min_value=min_date,
                               max_value=max_date - timedelta(days=7))
    # ë ë‚ ì§œë¥¼ ì„ íƒí•  ë•Œ ìµœì†Œ ë‚ ì§œëŠ” ì‹œì‘ ë‚ ì§œì´ë©°, ìµœëŒ€ ë‚ ì§œëŠ” 90ì¼ ì´ì „ê¹Œì§€ë¡œ ì œí•œ
    end_date = st.date_input("ë ë‚ ì§œ",
                             value=datetime(2022,6,15),
                             min_value=start_date + timedelta(days=7),
                             max_value=start_date + timedelta(days=60))
with col2:
    media = st.selectbox('ë§¤ì²´',('ì‹ë¬¼ê°¤ëŸ¬ë¦¬', 'ì‹ë¬¼ë³‘ì›', 'ë„¤ì´ë²„ì¹´í˜', 'ë„¤ì´ë²„ë¸”ë¡œê·¸', 'ë„¤ì´ë²„í¬ìŠ¤íŠ¸'), help="í™•ì¸í•˜ê³  ì‹¶ì€ ì™¸ë¶€ ë°ì´í„°ì˜ ë§¤ì²´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
with col3:
    temp_effect_size = st.slider('ì˜í–¥ë„ ë³¼ë¥¨', 0, 100, 83, help="ê° ë§¤ì²´ë³„ ì½˜í…ì¸ ì˜ ë°˜ì‘ë„ë¥¼ ì ìˆ˜í™”í•œ ê°’ì…ë‹ˆë‹¤. 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì˜í–¥ë„ê°€ ë†’ìŠµë‹ˆë‹¤.")
    effect_size = (100-int(temp_effect_size))/100
standard_df, new_df = extract_df(df, media, start_date, end_date, effect_size)

##ì¸í’‹ í•„í„° - ì›Œë“œí´ë¼ìš°ë“œìš©##
expander = st.expander('ì›Œë“œ í´ë¼ìš°ë“œ ì„¸ë¶€í•„í„°')
with expander:
    col1, col2= st.beta_columns(2)    
    with col1:
        type = st.selectbox('ê¸°ì¤€',('ë‹¨ìˆœ ë¹ˆë„(Countvectorizer)','ìƒëŒ€ ë¹ˆë„(TF-IDF)'), 
                            help="""ë‹¨ìˆœë¹ˆë„ë€ ë¬¸ì„œ ë‚´ ê° ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚œ ë¹ˆë„ ì¦‰, ë‚˜íƒ€ë‚œ íšŸìˆ˜ë¥¼ ì„¸ì„œ ë§Œë“  ê°’ì…ë‹ˆë‹¤. 
                            ìƒëŒ€ë¹ˆë„ëŠ” ë‹¨ì–´ê°€ ë¬¸ì„œ ë‚´ì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.""")
    with col2:
        keyword_no = st.number_input("í‚¤ì›Œë“œ ë³¼ë¥¨", value=100, min_value=1, step=1,
                                     help="ì›Œë“œ í´ë¼ìš°ë“œë¥¼ í†µí•´ ë³´ê³  ì‹¶ì€ ë‹¨ì–´ì˜ ê°¯ìˆ˜ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")   
    stopwords = st_tags(
        label = 'ì œê±°í•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”',
        value = ['ì‹ë¬¼', 'í™”ë¶„'],
        suggestions = ['ì‹ë¬¼', 'í™”ë¶„'],
        key = '1')

##ì›Œë“œ í´ë¼ìš°ë“œ##
def get_tfidf_top_words(df, keyword_no):
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    tfidf_top_words = dict(tfidf_top_words)
    return tfidf_top_words

def get_count_top_words(df, keyword_no):
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df['ì œëª©+ë‚´ìš©(nng)'].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(keyword_no).to_dict()
    return count_top_words

try :
    if type == 'ë‹¨ìˆœ ë¹ˆë„(Countvecterize)' :
        words = get_count_top_words(standard_df, keyword_no)
    else :
        words = get_tfidf_top_words(standard_df, keyword_no)

    #ì›Œë“œí´ë¼ìš°ë“œ
    wc = WordCloud(background_color="white", colormap='Spectral', contour_color='steelblue', font_path="/app/streamlit/font/Pretendard-Bold.otf")
    wc.generate_from_frequencies(words)

    ## ë™ì  ì›Œë“œ í´ë¼ìš°ë“œ ##
    # ì»¬ëŸ¬ íŒ”ë ˆíŠ¸ ìƒì„±
    word_list = []
    freq_list = []
    fontsize_list = []
    position_list = []
    orientation_list = []
    color_list = []

    for (word, freq), fontsize, position, orientation, color in wc.layout_:
        word_list.append(word)
        freq_list.append(freq)
        fontsize_list.append(fontsize)
        position_list.append(position)
        orientation_list.append(orientation)
        color_list.append(color)

    # get the positions
    x = []
    y = []
    for i in position_list:
        x.append(i[0])
        y.append(i[1])

    # WordCloud ì‹œê°í™”ë¥¼ ìœ„í•œ Scatter Plot ìƒì„±
    hover_text = word_list  # í‚¤ì›Œë“œë§Œ í¬í•¨í•œ í…ìŠ¤íŠ¸ ìƒì„±

    # ìˆ«ìê°’ì„ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ hovertemplate ì„¤ì •
    hover_template = "<b>%{text}</b><br>Count: %{customdata[0]}"

    fig = go.Figure(go.Scatter(
        x=x, y=y, mode="text",
        text=hover_text,
        customdata=list(zip(freq_list)),  # ìˆ«ìê°’ì„ customdataë¡œ ì „ë‹¬
        hovertemplate=hover_template,  # hovertemplate ì„¤ì •
        textfont=dict(size=fontsize_list, color=color_list),
    ))

    fig.update_layout(
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        hovermode='closest'
    )

    st.plotly_chart(fig, use_container_width=True)

except :
    st.warning('ì˜í–¥ë„ ë²”ìœ„ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”! ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤')    

#########Section2 - í‚¤ì›Œë“œ íë ˆì´íŒ…############
st.markdown("---")
st.markdown("<h2 id='section2'>ğŸ’ í‚¤ì›Œë“œ íë ˆì´ì…˜</h2>", unsafe_allow_html=True)
def new_keyword(standard_df, new_df):
    df['ì œëª©+ë‚´ìš©(nng)'] = df['ì œëª©+ë‚´ìš©(nng)'].map(to_list)
    content_list_1 = []
    content_list_1.extend(list(itertools.chain.from_iterable([eval(i) for i in standard_df['ì œëª©+ë‚´ìš©(nng)']])))
    content_list_2 = []
    content_list_2.extend(list(itertools.chain.from_iterable([eval(i) for i in new_df['ì œëª©+ë‚´ìš©(nng)']])))

    new_keywords = set(content_list_2) - set(content_list_1)   
    result_dict = {}
    # ì´ë²ˆë‹¬ì—ë§Œ ìˆëŠ” 
    for word in new_keywords:
        word_df = new_df[new_df['ì œëª©+ë‚´ìš©(nng)'].str.contains(word)]
        if len(word_df) > 0:
            avg_views = word_df['ì˜í–¥ë„'].mean()
            urls = word_df['URL'].tolist()
            result_dict[word] = {'í‰ê·  ì˜í–¥ë„': round(float(avg_views), 2), 'URL': urls}
            
    # ì¡°íšŒìˆ˜ ë†’ì€ìˆœìœ¼ë¡œ ì •ë ¬        
    result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1]['í‰ê·  ì˜í–¥ë„'], reverse=True))    

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    keywords = []
    avg_views = []
    urls = []
    
    for key, value in result_dict.items():
        keywords.append(key)
        avg_views.append(value['í‰ê·  ì˜í–¥ë„'])
        urls.append('\n'.join(value['URL']))
    
    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'í‰ê·  ì˜í–¥ë„': avg_views,
        'URL': urls
    })

    return result_df

def rising_keyword(standard_df, new_df):
    # ë°ì´í„° í•©ì¹˜ê¸° 
    df = pd.concat([standard_df, new_df])

    # ë‚ ì§œ êµ¬í•˜ê¸°
    ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚  = df['ë‚ ì§œ'].max()
    ì´ë²ˆì£¼ì²«ë‚  = (df['ë‚ ì§œ'].max() - timedelta(days=7))
    ì§€ë‚œì£¼ì²«ë‚  = ì´ë²ˆì£¼ì²«ë‚  - timedelta(days=7)
    
    ì´ë²ˆì£¼_df = df[(df['ë‚ ì§œ'] > ì´ë²ˆì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ë§ˆì§€ë§‰ë‚ )]
    ì§€ë‚œì£¼_df = df[(df['ë‚ ì§œ'] > ì§€ë‚œì£¼ì²«ë‚ ) & (df['ë‚ ì§œ'] <= ì´ë²ˆì£¼ì²«ë‚ )]
        
    # ì¤‘ë³µê°’ ì œê±°í•œ ìƒˆë¡œìš´ ì—´ ì¶”ê°€
    ì´ë²ˆì£¼_df = ì´ë²ˆì£¼_df.copy()
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì´ë²ˆì£¼_df['unique_content'] = ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    ì§€ë‚œì£¼_df = ì§€ë‚œì£¼_df.copy()
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['ì œëª©+ë‚´ìš©(nng)'].apply(lambda x: ast.literal_eval(x))
    ì§€ë‚œì£¼_df['unique_content'] = ì§€ë‚œì£¼_df['unique_content'].apply(lambda x: list(set(x)))

    this_week_words = list(ì´ë²ˆì£¼_df['unique_content'].explode())
    last_week_words = list(ì§€ë‚œì£¼_df['unique_content'].explode())

    this_week_word_counts = Counter(this_week_words)
    last_week_word_counts = Counter(last_week_words)

    # ì´ë²ˆì£¼ì™€ ì§€ë‚œì£¼ì— ëª¨ë‘ ì–¸ê¸‰ëœ ë‹¨ì–´ë¥¼ ëª¨ì€ ì§‘í•©
    common_words = set(this_week_word_counts.keys()) & set(last_week_word_counts.keys())
    result = {}
    for word in common_words:
        # í•´ë‹¹ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ëª¨ë“  URLì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëª¨ìŒ
        url_list = list(ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['unique_content'].apply(lambda x: word in x)]['URL'])
        # ì˜í–¥ë„ê°€ ê°€ì¥ ë†’ì€ URLì„ ì°¾ì•„ì„œ ì¶œë ¥
        url = max(url_list, key=lambda x: ì´ë²ˆì£¼_df.loc[ì´ë²ˆì£¼_df['URL'] == x, 'ì˜í–¥ë„'].iloc[0])
        increase_rate = (this_week_word_counts[word] - last_week_word_counts[word]) / this_week_word_counts[word]
        result[word] = {'ìƒìŠ¹ë¥ ': round(increase_rate, 2), 'URL': url}

    # ìƒìŠ¹ë¥  ê¸°ì¤€ ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
    keywords = []
    ups = []
    urls = []

    for word, data in sorted(result.items(), key=lambda x: x[1]['ìƒìŠ¹ë¥ '], reverse=True):
        if data['ìƒìŠ¹ë¥ ']>0:
            keywords.append(word)
            ups.append(f"{data['ìƒìŠ¹ë¥ ']*100}%")
            urls.append(data['URL'])

    result_df = pd.DataFrame({
        'í‚¤ì›Œë“œ': keywords,
        'ìƒìŠ¹ë¥ ': ups,
        'URL': urls
    })

    if len(result_df.index) >= 1 :
        return result_df

##í‚¤ì›Œë“œ##
try:
    new_keyword = new_keyword(standard_df, new_df)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ì‹ ê·œ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

try:
    rising_keyword = rising_keyword(standard_df, new_df)
except:
    st.warning("âš ï¸ í•´ë‹¹ ê¸°ê°„ ë™ì•ˆ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

##ì‹ ê·œ í‚¤ì›Œë“œ##
grouped_new_keyword = new_keyword.groupby('URL')
key_counter = 1
new_html_tags = ''
for url, group in grouped_new_keyword:
    keywords = ' | '.join(group['í‚¤ì›Œë“œ'])
    percent = group['í‰ê·  ì˜í–¥ë„'].iloc[0]
    key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
    new_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ’«)</b>&nbsp;"

##ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ##    
grouped_rising_keyword = rising_keyword.groupby('URL')
key_counter = 1
rising_html_tags = ''
for url, group in grouped_rising_keyword:
    keywords = ' | '.join(group['í‚¤ì›Œë“œ'])
    percent = group['ìƒìŠ¹ë¥ '].iloc[0]
    key_counter = (key_counter % 4) + 1  # Reset key counter after reaching 4
    rising_html_tags += f"<a id='key{key_counter}' href='{url}'>{keywords}</a><b>({percent}ğŸ”¥)</b>&nbsp;"

#HTML
st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
st.markdown(f"""
    <h3>ì‹ ê·œ í‚¤ì›Œë“œâ­ï¸</h3>
    <div class='callout'>
    {new_html_tags}
    </div>""",
    unsafe_allow_html=True
)
st.markdown(f"""
    <h3>ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œğŸ“ˆ</h3>
    <div class='callout'>
    {rising_html_tags}
    </div>""",
    unsafe_allow_html=True
)

#########Section3 - í‚¤ì›Œë“œ deepdive(ì‹œê³„ì—´)############
st.markdown("---")
st.markdown("<h2 id='section3'>â³ ì‹œê¸°ë³„ í‚¤ì›Œë“œ ì˜í–¥ë„</h2>", unsafe_allow_html=True)
df2 = pd.read_csv('/app/streamlit/data/df_á„á…³á„…á…¦á†«á„ƒá…³_github.csv')
df2['ë‚ ì§œ'] = pd.to_datetime(df2['ë‚ ì§œ'])

col1, col2 = st.beta_columns((0.2, 0.8))
with col1:
    keyword1 = st.text_input('ê¶ê¸ˆí•œ í‚¤ì›Œë“œ', value='ì œë¼ëŠ„')
with col2:
    keyword2 = st_tags(
        label = 'ë¹„êµí•  í‚¤ì›Œë“œ',
        text = 'ì§ì ‘ ì…ë ¥í•´ë³´ì„¸ìš”(ìµœëŒ€ 5ê°œ)',
        value = ['ìŠ¤í‚¨ë‹µì„œìŠ¤'],
        maxtags = 5,
        key = '2')

def get_df(df, word1, args):
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
    result = df[(df['ë§¤ì²´'] == 'ì‹ë¬¼ê°¤ëŸ¬ë¦¬') | (df['ë§¤ì²´'] == 'ì‹ë¬¼ë³‘ì›')]
    result = result[(result['ë‚ ì§œ'] >= '2022-04-27') & (result['ë‚ ì§œ'] <= '2023-04-26')]
    keywords = [word1] + (args)
    result = result[result['ì œëª©+ë‚´ìš©(nng)'].str.contains('|'.join(keywords))]
    for arg in keywords:
        if arg not in ' '.join(result['ì œëª©+ë‚´ìš©(nng)'].tolist()):
            st.warning(f"'ë‹¤ìŒ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. {arg}'")
            return None, None
    return result, keywords

def deepdive_lineplot(df, keywords):
    # í‚¤ì›Œë“œë³„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    keywords = keywords[::-1]
    keyword_dfs = {}
    for keyword in keywords:
        keyword_dfs[keyword] = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)].copy()
    
    # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í•‘í•˜ê³  ì˜í–¥ë„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
    impact_by_week = {}
    for keyword, keyword_df in keyword_dfs.items():
        keyword_df['ë‚ ì§œ'] = pd.to_datetime(keyword_df['ë‚ ì§œ'])
        keyword_df.set_index('ë‚ ì§œ', inplace=True)
        impact_by_week[keyword] = keyword_df.resample('W')['ì˜í–¥ë„'].mean()

    # ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    colors = ["grey"] * (len(keywords) - 1) + ["blue"]

    for i, (keyword, impact) in enumerate(impact_by_week.items()):
        fig.add_trace(go.Scatter(x=impact.index, y=impact.values, name=keyword, line_color=colors[i]), secondary_y=False)
        
    fig.update_layout(yaxis_title="í‰ê·  ì˜í–¥ë„")
    st.plotly_chart(fig, use_container_width=True)

def get_TOP_10(df, keyword):
    temp_df = df[df['ì œëª©+ë‚´ìš©(nng)'].str.contains(keyword)]
    top10_list = []
    for media_category in temp_df['ë§¤ì²´'].unique():
        df_category = temp_df[temp_df['ë§¤ì²´'] == media_category]
        if len(df_category) > 0:
            try:
                band_top10 = df_category.nlargest(10, 'ì˜í–¥ë„')
                band_top10['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                band_top10 = band_top10.reset_index(drop=True)
                band_top10 = band_top10[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(band_top10)
            except ValueError:
                df_category['ì˜í–¥ë„'] *= 100  # ì˜í–¥ë„ë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
                df_category = df_category.reset_index(drop=True)
                df_category = df_category[['ë§¤ì²´', 'ì‘ì„±ì', 'ì œëª©', 'URL', 'ì˜í–¥ë„']]
                top10_list.append(df_category)
    if len(top10_list) > 0:
        return pd.concat(top10_list, ignore_index=False)
    else:
        return None
    
try :
    st.markdown(f"<style>{STYLE}</style>", unsafe_allow_html=True)
    st.markdown("<h3>í‚¤ì›Œë“œë³„ ì˜í–¥ë„ ê·¸ë˜í”„</h3>", unsafe_allow_html=True)
    deepdive_df, deepdive_keywords = get_df(df2, keyword1, keyword2)
    deepdive_lineplot(deepdive_df, deepdive_keywords)
    st.markdown("<h3>ë§¤ì²´ë³„ Top10 ê²Œì‹œê¸€</h3>", unsafe_allow_html=True)
    keyword_result = get_TOP_10(df2, keyword1)
    st.dataframe(keyword_result)

except :
    st.warning("í•´ë‹¹ í‚¤ì›Œë“œì— ëŒ€í•œ ê²°ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")


#########Section4 - í‚¤ì›Œë“œ deepdive(ë„¤íŠ¸ì›Œí¬ ë¶„ì„)############
st.markdown("---")
st.markdown("<h2 id='section4'>í‚¤ì›Œë“œ ì—°ê´€íƒìƒ‰</h2>", unsafe_allow_html=True)

all_keywords = [keyword1]+keyword2
st.text(f'ğŸ”® {all_keywords}ì— ëŒ€í•œ ì—°ê´€ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤')

#ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê²°ê³¼
def ë„¤íŠ¸ì›Œí¬(network_list, all_keywords):
    networks = []
    for review in network_list:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=50)

    G = nx.Graph(font_path='/app/streamlit/font/Pretendard-Bold.otf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in all_keywords:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=15)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in all_keywords:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", edge_color="grey", width=edge_weights)

    net = Network(notebook=True, cdn_resources='in_line')
    net.from_nx(G)
    return [net, similar_words]

#ì—°ê´€ë¶„ì„
if st.button('ë¶„ì„ì„ ì‹œì‘í•˜ê¸°'):
    with st.spinner('ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        all_keywords = [keyword1] + keyword2
        network_list = [eval(i) for i in df2['ì œëª©+ë‚´ìš©(nng)']]
        ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(network_list, all_keywords)
        if ë„¤íŠ¸ì›Œí¬ is not None:
            try:
                net = ë„¤íŠ¸ì›Œí¬[0]
                net.save_graph('/app/streamlit/pyvis_graph.html')
                HtmlFile = open('/app/streamlit/pyvis_graph.html', 'r', encoding='utf-8')
                components.html(HtmlFile.read(), height=600)
            except:
                st.warning('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')
        else:
            st.warning('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')
